{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from diffusers import AutoencoderKL\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "class Subject200KDateset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_dataset,\n",
    "        condition_size: int = 512,\n",
    "        target_size: int = 512,\n",
    "        image_size: int = 512,\n",
    "        padding: int = 0,\n",
    "        condition_type: str = \"subject\",\n",
    "        drop_text_prob: float = 0.1,\n",
    "        drop_image_prob: float = 0.1,\n",
    "        return_pil_image: bool = False,\n",
    "    ):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.condition_size = condition_size\n",
    "        self.target_size = target_size\n",
    "        self.image_size = image_size\n",
    "        self.padding = padding\n",
    "        self.condition_type = condition_type\n",
    "        self.drop_text_prob = drop_text_prob\n",
    "        self.drop_image_prob = drop_image_prob\n",
    "        self.return_pil_image = return_pil_image\n",
    "\n",
    "        self.to_tensor = T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # If target is 0, left image is target, right image is condition\n",
    "        # target = idx % 2\n",
    "        item = self.base_dataset[idx // 2]\n",
    "\n",
    "        # Crop the image to target and condition\n",
    "        image = item[\"image\"]\n",
    "        left_img = image.crop(\n",
    "            (\n",
    "                self.padding,\n",
    "                self.padding,\n",
    "                self.image_size + self.padding,\n",
    "                self.image_size + self.padding,\n",
    "            )\n",
    "        )\n",
    "        right_img = image.crop(\n",
    "            (\n",
    "                self.image_size + self.padding * 2,\n",
    "                self.padding,\n",
    "                self.image_size * 2 + self.padding * 2,\n",
    "                self.image_size + self.padding,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Resize the images\n",
    "        left_img = left_img.resize(\n",
    "            (self.condition_size, self.condition_size)\n",
    "        ).convert(\"RGB\")\n",
    "        right_img = right_img.resize(\n",
    "            (self.target_size, self.target_size)\n",
    "        ).convert(\"RGB\")\n",
    "\n",
    "\n",
    "        description_0 = item[\"description\"][\"description_0\"]\n",
    "        description_1 = item[\"description\"][\"description_1\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"left_image\": left_img,\n",
    "            \"right_image\": right_img,\n",
    "            \"condition_type\": self.condition_type,\n",
    "            \"description_0\": description_0,\n",
    "            \"description_1\": description_1,\n",
    "            # **({\"pil_image\": image} if self.return_pil_image else {}),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=16): 100%|██████████| 206841/206841 [00:33<00:00, 6205.68 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('Yuanshi/Subjects200K')\n",
    "def filter_func(item):\n",
    "    if not item.get(\"quality_assessment\"):\n",
    "        return False\n",
    "    return all(\n",
    "        item[\"quality_assessment\"].get(key, 0) >= 5\n",
    "        for key in [\"compositeStructure\", \"objectConsistency\", \"imageQuality\"]\n",
    "    )\n",
    "\n",
    "data_valid = dataset[\"train\"].filter(\n",
    "    filter_func,\n",
    "    num_proc=16,\n",
    "    cache_file_name=\"./cache/dataset/data_valid.arrow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2026/239934 [03:44<7:19:45,  9.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/ptca/lib/python3.10/site-packages/PIL/ImageFile.py:515\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[1;32m    516\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Save target image\u001b[39;00m\n\u001b[1;32m     34\u001b[0m target_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/target_images/target_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget_image\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_image_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Save condition image\u001b[39;00m\n\u001b[1;32m     38\u001b[0m condition_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/condition_images/condition_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/ptca/lib/python3.10/site-packages/PIL/Image.py:2432\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2429\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2432\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2433\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m/opt/conda/envs/ptca/lib/python3.10/site-packages/PIL/PngImagePlugin.py:1407\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode, default_image, append_images)\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1407\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m/opt/conda/envs/ptca/lib/python3.10/site-packages/PIL/ImageFile.py:519\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    517\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 519\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    521\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[0;32m/opt/conda/envs/ptca/lib/python3.10/site-packages/PIL/ImageFile.py:538\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    539\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the dataset\n",
    "training_config = {\n",
    "    \"dataset\": {\n",
    "        \"condition_size\": 512,\n",
    "        \"target_size\": 512,\n",
    "        \"image_size\": 512,\n",
    "        \"padding\": 8,\n",
    "        \"drop_text_prob\": 0.1,\n",
    "        \"drop_image_prob\": 0.1,\n",
    "    },\n",
    "    \"condition_type\": \"subject\",\n",
    "}\n",
    "\n",
    "subject_dataset = Subject200KDateset(\n",
    "    data_valid,\n",
    "    condition_size=training_config[\"dataset\"][\"condition_size\"],\n",
    "    target_size=training_config[\"dataset\"][\"target_size\"],\n",
    "    image_size=training_config[\"dataset\"][\"image_size\"],\n",
    "    padding=training_config[\"dataset\"][\"padding\"],\n",
    "    condition_type=training_config[\"condition_type\"],\n",
    "    drop_text_prob=training_config[\"dataset\"][\"drop_text_prob\"],\n",
    "    drop_image_prob=training_config[\"dataset\"][\"drop_image_prob\"],\n",
    ")\n",
    "\n",
    "# Create directories to save images\n",
    "os.makedirs(\"output/left_images\", exist_ok=True)\n",
    "os.makedirs(\"output/right_images\", exist_ok=True)\n",
    "os.makedirs(\"output/metadata\", exist_ok=True)\n",
    "# Save target and condition images\n",
    "for idx in tqdm(range(len(subject_dataset))):\n",
    "    item = subject_dataset[idx]\n",
    "\n",
    "    # Save target image\n",
    "    left_images_path = f\"output/left_images/left_{idx}.png\"\n",
    "    item[\"left_image\"].save(left_images_path)\n",
    "\n",
    "    # Save condition image\n",
    "    right_images_path = f\"output/condition_images/condition_{idx}.png\"\n",
    "    item[\"right_image\"].save(right_images_path)\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = {\n",
    "        \"description_0\": item[\"description_0\"],\n",
    "        \"description_1\": item[\"description_1\"],\n",
    "        \"collection\": data_valid[idx // 2][\"collection\"],\n",
    "        \"quality_assessment\": data_valid[idx // 2][\"quality_assessment\"],\n",
    "        \"target_image_path\": left_images_path,\n",
    "        \"condition_image_path\": right_images_path,\n",
    "    }\n",
    "\n",
    "    # Save metadata as JSON\n",
    "    metadata_path = f\"output/metadata/meta_{idx}.json\"\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "    break\n",
    "    # if idx % 100 == 0:\n",
    "    #     print(f\"Saved {idx} images.\")\n",
    "\n",
    "print(\"✅ Image saving completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Process and save images, latents, and metadata\n",
    "for idx, item in enumerate(filtered_dataset):\n",
    "    # Save the original image\n",
    "    image = item['image']\n",
    "    image_path = f\"output/images/image_{idx}.png\"\n",
    "    image.save(image_path)\n",
    "\n",
    "    # Preprocess image\n",
    "    image_tensor = preprocess_image(image)\n",
    "\n",
    "    # # Extract VAE latents\n",
    "    # latents = extract_latents(image_tensor, vae)\n",
    "    # np.save(f\"output/latents/latent_{idx}.npy\", latents)\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"collection\": item['collection'],\n",
    "        \"quality_assessment\": item['quality_assessment'],\n",
    "        \"description\": item['description'],\n",
    "        \"image_path\": image_path,\n",
    "    }\n",
    "    with open(f\"output/metadata/meta_{idx}.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx} samples.\")\n",
    "\n",
    "print(\"✅ Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from diffusers import AutoencoderKLCogVideoX\n",
    "# import decord\n",
    "# from decord import VideoReader\n",
    "from multiprocessing import Process, Queue, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(queue, progress_queue, vae_model_path, max_frames, width, height, gpu_id, output_dir, fps):\n",
    "    \"\"\"\n",
    "    Process videos assigned to a specific GPU.\n",
    "    \"\"\"\n",
    "    device = f\"cuda:{gpu_id}\"\n",
    "\n",
    "    # Load the VAE model\n",
    "    vae = AutoencoderKLCogVideoX.from_pretrained(vae_model_path, subfolder=\"vae\")\n",
    "    vae.to(device)\n",
    "    vae.eval()\n",
    "\n",
    "    while True:\n",
    "        video_path = queue.get()\n",
    "        if video_path is None:  # End signal\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Load video using Decord\n",
    "            # decord.bridge.set_bridge(\"native\")\n",
    "            # vr = VideoReader(video_path, ctx=decord.cpu(0))\n",
    "\n",
    "            # Calculate frame interval\n",
    "            # original_fps = 8\n",
    "            # frame_interval = int(original_fps / fps)\n",
    "\n",
    "            # # Extract frames\n",
    "            # # frames = vr.get_batch(range(0, min(len(vr), max_frames * frame_interval), frame_interval)).asnumpy()\n",
    "            # frames = vr.get_batch(range(0, min(len(vr), max_frames * frame_interval), frame_interval)).asnumpy()\n",
    "\n",
    "            # # Ensure exact number of frames\n",
    "            # if frames.shape[0] < max_frames:\n",
    "            #     pad_frames = max_frames - frames.shape[0]\n",
    "            #     print('>> shorter than max_frames : doing padding')\n",
    "            #     frames = np.pad(frames, ((0, pad_frames), (0, 0), (0, 0), (0, 0)), mode=\"constant\")\n",
    "            # elif frames.shape[0] > max_frames:\n",
    "            #     frames = frames[:max_frames]\n",
    "            # # Resize frames using OpenCV\n",
    "            # frames = np.array([cv2.resize(frame, (width, height), interpolation=cv2.INTER_LINEAR) for frame in frames])\n",
    "            frames = cv2.imread(video_path) \n",
    "            frames = cv2.resize(frames, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "            frames = np.expand_dims(frames, axis=0)  # Add frame dimension# single frame so open with cv2\n",
    "            # frames = np.array([cv2.resize(frames, (width, height), interpolation=cv2.INTER_LINEAR)])\n",
    "            frames = np.array(frames)\n",
    "            # Convert to torch tensor and preprocess\n",
    "            frames = torch.from_numpy(frames).float() / 255.0 * 2.0 - 1.0  # Normalize [-1, 1]\n",
    "            frames = frames.permute(0, 3, 1, 2)  # [F, H, W, C] -> [F, C, H, W]\n",
    "            \n",
    "            # Add batch dimension and permute for VAE\n",
    "            frames = frames.unsqueeze(0).permute(0, 2, 1, 3, 4).to(device)  # [B, C, F, H, W]\n",
    "\n",
    "            # Encode video to latent space\n",
    "            with torch.no_grad():\n",
    "                latent_dist = vae.encode(frames).latent_dist\n",
    "                latents = latent_dist.sample() * vae.config.scaling_factor\n",
    "\n",
    "            # Save latents\n",
    "            output_path = os.path.join(output_dir, Path(video_path).stem + \"_vae_latents.npy\")\n",
    "            np.save(output_path, latents.cpu().numpy())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path}: {e}\")\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Notify progress\n",
    "        progress_queue.put(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vae_latents(\n",
    "    video_paths, vae_model_path, output_dir, height=480, width=720, max_frames=49, fps=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract VAE latents using multiple GPUs with controlled processes.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Detect available GPUs\n",
    "    available_gpus = list(range(torch.cuda.device_count()))\n",
    "    if not available_gpus:\n",
    "        raise RuntimeError(\"No GPUs are available!\")\n",
    "\n",
    "    print(f\"Using GPUs: {available_gpus}\")\n",
    "\n",
    "    # Create a process for each GPU\n",
    "    queues = [Queue() for _ in available_gpus]\n",
    "    progress_queue = Queue()\n",
    "    processes = []\n",
    "\n",
    "    for gpu_id, queue in zip(available_gpus, queues):\n",
    "        process = Process(target=process_video, args=(queue, progress_queue, vae_model_path, max_frames, width, height, gpu_id, output_dir, fps))\n",
    "        process.start()\n",
    "        processes.append(process)\n",
    "\n",
    "    # Distribute videos to queues\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        queues[i % len(available_gpus)].put(video_path)\n",
    "\n",
    "    # Send termination signals\n",
    "    for queue in queues:\n",
    "        queue.put(None)\n",
    "\n",
    "    # Track progress using tqdm\n",
    "    with tqdm(total=len(video_paths), desc=\"Extracting VAE latents\") as pbar:\n",
    "        completed = 0\n",
    "        while completed < len(video_paths):\n",
    "            progress_queue.get()  # Wait for progress notification\n",
    "            completed += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for process in processes:\n",
    "        process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPUs: [0, 1, 2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting VAE latents:   0%|          | 54/239944 [00:11<14:00:16,  4.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m video_paths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_dir2, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(video_dir2) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)][:\u001b[38;5;241m10\u001b[39m] \u001b[38;5;66;03m# single frame video (image)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Extract VAE latents\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mextract_vae_latents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTHUDM/CogVideoX-5b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/latents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# fps=8,\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mextract_vae_latents\u001b[0;34m(video_paths, vae_model_path, output_dir, height, width, max_frames, fps)\u001b[0m\n\u001b[1;32m     36\u001b[0m completed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m completed \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(video_paths):\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for progress notification\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     completed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     40\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/cogvideo_vanilla/lib/python3.12/multiprocessing/queues.py:103\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock:\n\u001b[0;32m--> 103\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sem\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/cogvideo_vanilla/lib/python3.12/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m/opt/conda/envs/cogvideo_vanilla/lib/python3.12/multiprocessing/connection.py:430\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 430\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/cogvideo_vanilla/lib/python3.12/multiprocessing/connection.py:395\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    393\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 395\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_dir1 = \"output/left_images\"\n",
    "video_dir2 = \"output/right_images\"\n",
    "video_paths = [os.path.join(video_dir1, f) for f in os.listdir(video_dir1) if f.endswith(\".png\")] # single frame video (image)\n",
    "video_paths += [os.path.join(video_dir2, f) for f in os.listdir(video_dir2) if f.endswith(\".png\")][:10] # single frame video (image)\n",
    "\n",
    "# Extract VAE latents\n",
    "extract_vae_latents(\n",
    "    video_paths,\n",
    "    vae_model_path=\"THUDM/CogVideoX-5b\",\n",
    "    output_dir=\"output/latents\",\n",
    "    height=512,\n",
    "    width=512,\n",
    "    max_frames=1,\n",
    "    # fps=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogvideo_vanilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
